Integrated Architectures for Semantic Clustering and Citation-Graph Analysis of Scientific Literature
The contemporary landscape of scientific research is characterized by an unprecedented volume of publication, particularly within the arXiv repository, which hosts millions of preprints across diverse disciplines such as physics, mathematics, and computer science. Traditional lexical search mechanisms, while foundational, increasingly fail to capture the nuanced semantic relationships and structural dependencies inherent in academic discourse. To address this, the development of sophisticated retrieval and organization systems has shifted toward the integration of n-dimensional embeddings and citation-informed graph theory. By moving beyond Large Language Models (LLMs) to specialized, document-level encoders and high-performance search engines, the scientific community can build tools that provide more accurate, reproducible, and computationally efficient discovery platforms. These systems must handle heterogeneous inputs—ranging from arXiv identifiers to raw text and complex PDF structures—while offering granular control over similarity thresholds and citation aggregation.

The Paradigm Shift in Scientific Document Representation
The fundamental challenge in clustering arXiv papers semantically lies in the transition from word-level representation to whole-document encoding. While early methods relied heavily on Bag-of-Words or TF-IDF vectors, these approaches were limited by their inability to capture context or the semantic weight of terms relative to the scientific domain. The emergence of dense retrieval models, particularly those based on Transformer architectures, has enabled the mapping of scientific papers into n-dimensional spaces where proximity correlates with thematic relevance.   

Citation-Informed Embeddings and Triplet Loss
A critical advancement in this field is the Scientific Paper Embeddings using Citation-informed TransformERs (SPECTER) model. The core innovation of SPECTER is the use of the citation graph as an incidental supervision signal during pre-training. Standard language models, such as SciBERT, are trained on masked language modeling objectives, which often fail to capture the document-level relatedness required for effective clustering or recommendation. In contrast, SPECTER assumes that the act of citation signifies a latent semantic link.   

The model architecture utilizes a triplet loss objective to refine the embedding space. For any given query paper P 
Q
​
 , the system identifies a positive paper P 
+
​
  (one that P 
Q
​
  cites) and a negative paper P 
−
​
  (a random paper not cited by P 
Q
​
 ). The training goal is to optimize the Transformer's parameters so that the distance between P 
Q
​
  and P 
+
​
  in the embedding space is minimized, while the distance between P 
Q
​
  and P 
−
​
  is maximized. This results in embeddings that are naturally tuned to the structural realities of scientific citation, outperforming general-purpose models in tasks such as topic classification and citation prediction.   

Parameter-Efficient and Hierarchical Clustering
While document-level encoders provide the vectors, the task of clustering millions of papers requires parameter-efficient methodologies to remain computationally viable. The Franca project introduces Nested Matryoshka Clustering (NMC), a multi-head clustering projector that refines feature representations into increasingly fine-grained clusters without increasing the model's overall parameter count. This approach is particularly relevant for arXiv, where a researcher might need to navigate from a broad category like "Machine Learning" down to a specific sub-niche like "Parameter-Efficient Fine-Tuning."   

The integration of Relative Absolute Spatial Attention (RASA) further enhances these embeddings by explicitly removing positional biases. In scientific literature, where important terms may appear in different sections (title, abstract, or conclusion), RASA ensures that the semantic encoding is invariant to the spatial arrangement of the text, leading to more robust semantic clusters.   

Model Feature	SciBERT	SPECTER	SPECTER 2.0	Franca
Base Architecture	BERT-Base	SciBERT	SciBERT/XLMR	ViT/Transformer
Training Objective	Masked LM	Citation Triplet Loss	Multi-task Citation Loss	Matryoshka Clustering
Scientific Signal	Scivocab	Citation Graph	SciRepEval Benchmark	Uncurated Internet Data
Primary Use Case	NER, PICO	Document Similarity	Multi-format Discovery	Semantic Segmentation
Source ID				
  
Handling Heterogeneous Inputs: arXiv ID, Text, and PDF
A versatile clustering tool must accommodate the various ways researchers interact with literature. This requires a modular ingestion pipeline capable of resolving persistent identifiers, vectorizing unstructured text, and parsing structured bibliographic data from complex document formats.

Automated Metadata Resolution
For inputs provided as arXiv IDs, the system leverages the arXiv API or wrappers like arxiv.py to fetch titles and abstracts. However, to build a complete citation network, these identifiers must be resolved against global academic graphs. The Semantic Scholar API and OpenAlex are the primary engines for this resolution. Semantic Scholar's Academic Graph (S2AG) indexes over 225 million papers and 2.8 billion citations, providing a stable backbone for mapping a single ID to its entire citation and reference context.   

When a user provides raw text (such as a working title or abstract), the system utilizes semantic search endpoints. OpenAlex offers an AI-powered semantic search that allows users to find similar works based on text alone, returning a list of entities that can then be processed into the clustering engine.   

Deep PDF Parsing with GROBID
The most significant technical hurdle in ingestion is the processing of PDF inputs. Research papers are often stored as unstructured PDF files where references are locked in formatted text strings. To extract these, the open-source community relies on GROBID (GeneRation Of BIbliographic Data), a machine learning library that converts PDF headers and bibliographies into structured XML/TEI.   

The GROBID pipeline, as implemented in tools like the citation-graph-builder, identifies references within the body of a paper and maps them to their respective positions in the bibliography. This allows the clustering tool to build a "dependency graph" even for papers that have not yet been indexed by major databases, or for proprietary documents uploaded by the user. This extraction is memory-intensive, typically requiring 3-8 GB of RAM for batch processing, but it is essential for achieving the level of detail required for citation aggregation.   

High-Performance Search and Clustering Engines
Once scientific documents are converted into n-dimensional embeddings, the problem becomes one of computational geometry: how to find the k-nearest neighbors (k-NN) in a high-dimensional space efficiently.

USearch: Beyond FAISS for Scientific Vectors
While FAISS (Facebook AI Similarity Search) has long been the standard for vector search, USearch has emerged as a lightweight and faster alternative specifically designed for high-performance clustering. USearch implements the HNSW (Hierarchical Navigable Small World) algorithm but optimizes it using SIMD instructions like x86 AVX-512 and ARM SVE.   

For an arXiv-scale tool, USearch offers several critical advantages:

Portability: It is a single-header C++ library with no obligatory dependencies like BLAS or OpenMP, making it easier to integrate into modular architectures.   

Memory Efficiency: It allows for viewing large indexes from disk without loading the entire set into RAM, which is vital for managing the millions of vectors produced by the arXiv corpus.   

User-Defined Metrics: It supports custom distance functions, allowing the clustering engine to combine semantic cosine similarity with citation-based weightings.   

Near-Real-Time Clustering Logic
To provide a "user-specified number of similar results," the engine must allow the user to define the k parameter in a k-NN search. Projects like SCAN (Semantic Clustering by Adopting Nearest Neighbors) perform clustering in a three-step process: solving a pretext task (embedding), performing the clustering step (SCAN), and then self-labeling to refine the results. This methodology ensures that the clusters are not just mathematically close but semantically meaningful. In an interactive tool, this allows the user to input a "seed" paper and receive n similar results, where n is a parameter passed directly to the USearch search method.   

Feature	USearch	FAISS	SCAN	Franca
Speed/Performance	10x faster HNSW	Industry standard	Multi-step refinement	Parameter-efficient
Dependencies	Minimal (header-only)	High (BLAS, OpenMP)	PyTorch/Transformers	PyTorch/cuml/faiss
ID Support	32, 40, 64-bit	32, 64-bit	Local indices	Global feature heads
Clustering Style	Near-real-time	Batch	Self-labeling	Matryoshka
Source ID				
  
Citation Network Analysis and Aggregation
A core requirement for advanced research tools is the ability to move beyond similarity scores and provide structural insights into the citation landscape. This involves identifying "common citations" and "dependency graphs."

Common Citation Aggregation
Common citation aggregation is a technique used to identify foundational papers that multiple "papers of interest" share in their bibliographies. This is often referred to as bibliographic coupling or co-citation analysis depending on the direction of the links. If a researcher selects five papers (a "papers of interest" list), the tool should query the references for all five and find the intersection.   

This process is facilitated by APIs like Semantic Scholar's batch endpoint, which allows for retrieving citation and reference data for up to 1000 papers in a single request. By aggregating these results, the system can rank the references by how frequently they appear across the user's selected library. High-frequency references represent the "dependency nodes" that form the intellectual foundation of that research cluster.   

Dependency Graphs and Logical Structure
A "dependency graph" in this context serves as a visual and logical map of research evolution. Unlike a simple citation graph, a dependency graph implies a directional and often hierarchical relationship—where certain papers are prerequisites for understanding others.   

The Paperscape project provides a robust precedent for this, modeling the arXiv as a physical system where citations act as attractive forces. In this visualization, papers are nodes, and their size represents their citation impact. For a custom engine, the dependency graph should allow users to trace "backward citations" (foundational papers) and "forward citations" (derivative works), creating a navigable timeline of how a specific idea has propagated through the arXiv.   

Identifying Gaps in Current Open-Source Offerings
A comprehensive review of the research material suggests that while many tools exist, no single open-source project fulfills every requirement of the user's query.

Arxiv Sanity Preserver / Lite: Excellent for tracking new papers and providing TF-IDF similarity, but lacks citation-informed embeddings and citation aggregation.   

Inciteful: Strong on citation network analysis and identifying similar papers through graph theory, but does not natively handle PDF parsing or n-dimensional semantic clustering in the same interface.   

Connected Papers: Provides high-quality visual graphs of research landscapes but is a commercial platform (with an API) rather than a fully open-source, self-hostable clustering project.   

Paperscape: Offers an incredible global view of the arXiv based on citations, but is more of an interactive map than a utility for custom library clustering and user-specified recommendation.   

LitDB: Provides local library curation and semantic search via OpenAlex but lacks the visualization components for dependency graphs.   

The primary missing component is an integrated "orchestration layer" that can tie GROBID-based PDF parsing to a USearch-based vector engine, while using the Semantic Scholar or OpenAlex APIs for real-time citation aggregation and visualization.

Proposed Modular Architecture for a Custom Engine
To fulfill the requirements for a tool that clusters arXiv papers semantically using both citations and embeddings, a modular architecture is planned. This architecture is designed to be extensible, allowing for the integration of new models and data sources as they emerge.

Layer 1: The Ingestion and Resolution Module
This layer is responsible for normalizing all inputs into a common Paper Object.

ArXiv ID Handler: Uses arxiv.py to fetch metadata.   

PDF Processor: Deploys a GROBID Docker container to extract references and full text from uploaded files.   

API Bridge: Interfaces with OpenAlex and Semantic Scholar to fill in missing metadata, such as citation counts and journal venues.   

Layer 2: The Semantic Space and Embedding Module
This layer converts text and IDs into n-dimensional vectors.

Embedding Head: Uses the SPECTER 2.0 model (via the transformers library) to generate 768-dimensional document embeddings.   

Vector Store: USearch handles the indexing and k-NN retrieval. It allows for a "user-specified number of similar results" by exposing the k parameter in the API.   

Matryoshka Projector: (Optional) If the user requires hierarchical navigation, a Franca-style projector can be applied to create multi-scale clusters.   

Layer 3: The Citation Graph and Aggregation Engine
This layer builds the structural relationships between the papers.

Citation Aggregator: Fetches references for all papers in the 'papers of interest' list and computes the intersection to find common citations.   

Dependency Logic: Maps the chronological and logical flow of citations. It identifies "seed papers" (the ancestors of the cluster) and "derivative works" (the descendants).   

Layer 4: The Recommendation and Visualization Layer
This layer presents the data to the user.

Recommendation Logic: A hybrid ranker that combines cosine similarity (from Layer 2) with citation count and co-citation frequency (from Layer 3) to suggest new papers.

Visualization Engine: Uses Gephi-lite or D3.js to render an interactive "dependency graph" and a "semantic map". The semantic map uses dimensionality reduction (t-SNE or UMAP) to project the n-dimensional vectors into 2D space.   

Module	Primary Tool/Library	Functional Role
Ingestion	GROBID	
Extracting structure from PDFs 

Embedding	SPECTER 2.0	
Citation-informed vectorization 

Search	USearch	
High-speed k-NN and clustering 

Data Provider	OpenAlex API	
Global academic graph data 

Visualization	D3.js / Cytoscape	
Dependency graph rendering 

Local DB	Turso (libsql)	
'Papers of Interest' management 

  
Deep Insights into Semantic Similarity and Research Traversal
The integration of semantic similarity graphs (SSG) with traditional citation graphs offers a "third-order" insight into research trends. By moving through a semantic similarity graph, a researcher can find "sparse connections"—papers that are semantically identical but lack a direct citation link. This is particularly useful for interdisciplinary research, where different fields may use different terminology for the same concepts (e.g., "self-attention" in computer science vs. "dynamic weights" in neuroscience).   

Traversal Algorithms for Knowledge Discovery
Traditional RAG (Retrieval-Augmented Generation) systems often rely on mass retrieval via cosine similarity, which can introduce noise. A better approach for a scientific engine is "anchor-based traversal". In this model, the user "anchors" to a specific paper in their 'papers of interest' list. The algorithm then explores the neighborhood in the semantic similarity graph, prioritizing chunks or papers that have both high semantic similarity and strong structural (citation) ties to the anchor. This minimizes noise and ensures that the "dependency graph" reflects actual research lineage.   

Overcoming the Cold Start Problem
One of the most powerful implications of citation-informed embeddings like SPECTER is their ability to solve the "cold start" problem for new arXiv submissions. Because the model has learned the patterns of how titles and abstracts relate to citation positions, it can accurately place a brand-new, uncited paper into the global semantic map. This allows the custom engine to provide "similar papers" for a preprint that was uploaded only hours ago, a feat that citation-only tools like Paperscape or Inciteful cannot achieve.   

Technical Implementation of the 'Papers of Interest' Feature
The 'papers of interest' feature acts as a persistent user library. Projects like Paperlib and LitDB demonstrate how to implement this effectively using local databases.   

The implementation should follow a CRUD (Create, Read, Update, Delete) pattern:

Add: A user inputs an arXiv ID or uploads a PDF. The system triggers the Ingestion Module to fetch metadata and embeddings.   

Tag/Organize: Users can apply tags, which can then be used to filter the semantic clusters or color the dependency graph.   

Sync: Using the OpenAlex or Semantic Scholar APIs, the system can periodically check if "papers of interest" have received new citations or been published in journals, updating the dependency graph in real-time.   

Conclusion and Future Outlook
The development of a non-LLM, open-source engine for clustering arXiv papers semantically represents a critical step toward more disciplined and structural scientific discovery. By synthesizing the strengths of citation-informed Transformers like SPECTER 2.0 with the raw speed of vector engines like USearch and the structural depth of the OpenAlex graph, researchers can build tools that provide more than just search results; they provide maps of human knowledge.   

The proposed modular architecture addresses the current gap in the market by integrating PDF parsing (GROBID) with semantic clustering (SPECTER) and structural aggregation (S2AG/OpenAlex). This creates a system that is not only robust to different input formats but also provides the "dependency graphs" and "common citation" insights necessary for deep research. As scientific literature continues to grow, such tools will be indispensable for tracking the evolution of ideas and ensuring that foundational work is not lost in the noise of modern publication. The shift toward these integrated, modular, and open-source solutions ensures that the tools of discovery remain as accessible and transparent as the research they aim to organize.   

